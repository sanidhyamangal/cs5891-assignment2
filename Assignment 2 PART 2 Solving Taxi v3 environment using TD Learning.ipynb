{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26abf572",
   "metadata": {},
   "source": [
    "**Make sure to install `gym`, `numpy`, `matplotlib` and `tqdm` packages**\n",
    "\n",
    "If you are using conda package management you can use the following commands to install these packages.\n",
    "\n",
    "`pip install gym`; (Using `conda install -c conda-forge gym` can cause issues sometimes)\n",
    "\n",
    "`conda install tqdm`\n",
    "\n",
    "`conda install matplotlib`\n",
    "\n",
    "If you are using pip, use\n",
    "\n",
    "`pip install gym`\n",
    "\n",
    "`pip install tqdm`\n",
    "\n",
    "`pip install matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ce7446",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import trange\n",
    "from taxi_v3_agent import TaxiAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc532d",
   "metadata": {},
   "source": [
    "# [Taxi-v3](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py)\n",
    "\n",
    "Taxi-v3 is a reinforcement learning environment described in [Dietterich (2000)](https://arxiv.org/abs/cs/9905014) to demonstrate some issues with hierarchical reinforcement learning. The environment features a 5x5 grid with four locations denoted as red, blue, green and yellow. Our agent is a taxi started at any random position. The goal of the environment is to go pick up a passenger at an arbitrary location then send them to their destination as fast as possible.\n",
    "\n",
    "Let us first explore the different components of the environmen before go on to solve the problem\n",
    "\n",
    "**Make sure that you have installed `gym` python package and necessary dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f211ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c3f0b0",
   "metadata": {},
   "source": [
    "The state is a single integer from 0 to 499. This is a result of all combinations of:\n",
    "* 25 taxi positions\n",
    "* 5 possible locations of the passenger (including the case when the passenger is the taxi)\n",
    "* 4 destination locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cff611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of states\n",
    "print('The number of states in the Taxi environment: \\n')\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f21c55d",
   "metadata": {},
   "source": [
    "There are 6 discrete deterministic actions:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east \n",
    "- 3: move west \n",
    "- 4: pickup passenger\n",
    "- 5: dropoff passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af7dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of actions\n",
    "print('The number of possible actions in the Taxi environment: \\n')\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fcc33e",
   "metadata": {},
   "source": [
    "Reward\n",
    "\n",
    "* -1 for each action; to finish the task as fast as possible\n",
    "* +20 for delivering passenger\n",
    "* -10 for doing `pickup` (4) and `dropoff` (5) illegally\n",
    "* No penalty for illegal move (the taxi will remain at the same tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c66838",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'State: {env.reset()}')\n",
    "next_tuple = env.step(0)\n",
    "print(f'Action: 0, Next state: {next_tuple[0]}, Reward: {next_tuple[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8b80b",
   "metadata": {},
   "source": [
    "Rendering\n",
    "\n",
    "- blue: passenger\n",
    "- magenta: destination\n",
    "- yellow: empty taxi\n",
    "- green: full taxi\n",
    "- other letters: locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(mode = 'human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e013cc",
   "metadata": {},
   "source": [
    "# Implementing TD learning(On ie SARSA and Off ie Q-learning policies) to solve Taxi\n",
    "\n",
    "Our benchmark will be the final cumulative reward, from now on called `score`, at the end of an episode over time. For instance, when we do 1,000 episodes, as our agent becomes smarter, the cumulative rewards should get better and better. OpenAI uses 100-episode average score as their benchmark for \"solving\" the environment. **The solving score for this environment is 8.5.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37dfc89",
   "metadata": {},
   "source": [
    "## SARSA is the most basic of temporal difference learning. It got its name from the tuples we used for the learning:\n",
    "\n",
    "$$S_0 \\rightarrow A_0 \\rightarrow R_1 \\rightarrow S_1 \\rightarrow A_1 \\rightarrow R_2 \\rightarrow S_2 \\rightarrow ... \\rightarrow S_{t-1} \\rightarrow A_{t-1} \\rightarrow R_t \\rightarrow S_{t}$$\n",
    "\n",
    "Remember in Monte Carlo, you estimate the action values by:\n",
    "\n",
    "$$G_t = R_{t} + \\gamma R_{t+1} + ... $$\n",
    "\n",
    "$$Q(s_t,a_t) = \\frac{\\sum_{i=1}^{N(s_t,a_t)} G_i}{N(s_t,a_t)} = E[G(s_t,a_t)]$$\n",
    "\n",
    "where:\n",
    "* $G_t$ is discounted rewards\n",
    "* $R_t$ is the reward for each time step\n",
    "* $Q(s_t,a_t)$ is the action value for that state-action pair\n",
    "* $N(s_t,a_t)$ is the number of time we have seen that state-action pair\n",
    "\n",
    "We need to run the entire episode before updating Q because we need to calculate:\n",
    "\n",
    "$$G_t = R_{t} + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\gamma^3 R_{t+3} + ... $$\n",
    "\n",
    "But actually\n",
    "\n",
    "$$G_{t+1} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$$\n",
    "\n",
    "$$G_t = R_t + \\gamma (R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...) = R_t + \\gamma (G_{t+1}) = R_{t+1} + \\gamma Q(s_{t+1},a_{t+1})$$\n",
    "\n",
    "And since action value $Q(s,a)$ is **expected** discounted rewards, we can substitute $Q(s_{t+1},a_{t+1})$ for $G_{t+1}$. And with each time step, we update the action values as:\n",
    "\n",
    "$$Q(s_t,a_t) = Q(s_t,a_t) + \\alpha (R_{t+1} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t))$$\n",
    "\n",
    "where $\\alpha$ is the learning rate\n",
    "\n",
    "**Q1**\n",
    "\n",
    "**Implement**\n",
    "\n",
    "On policy TD Learning ie SARSA inside the `on_policy_td_sarsa` method inside the `taxi_v3_agent.py` file.\n",
    "\n",
    "Hints on implementation:\n",
    "\n",
    "For every **timestep**\n",
    "* **Step 1** Given state $s_t$, choose an action $a_t$ using epsilon greedy policy\n",
    "* **Step 2** Take that action and observe next state ($s_{t+1}$) and reward ($r_{t+1}$)\n",
    "* **Step 3** Update the action value $Q(s_t,a_t)$ using the equation above\n",
    "\n",
    "\n",
    "Then run the following code blocks to generate the outputs and plots. Discuss the output results in brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e257a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "a = TaxiAgent(env, gamma = 0.8, alpha = 1e-1, start_epsilon = 1, end_epsilon = 1e-2, epsilon_decay = 0.999)\n",
    "\n",
    "scores = []\n",
    "ts = []\n",
    "illegal_moves = []\n",
    "illegal_others = []\n",
    "moving_scores = []\n",
    "\n",
    "moving_nb = 100\n",
    "solved_score = 8.5\n",
    "start_time = datetime.now()\n",
    "\n",
    "for i in trange(500000):\n",
    "    #for the record\n",
    "    score = 0\n",
    "    t= 0\n",
    "    illegal_move = 0\n",
    "    illegal_other = 0\n",
    "    \n",
    "    #initiate state\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        \n",
    "        #get action\n",
    "        action = a.select_action(state, a.get_epsilon(i))\n",
    "        \n",
    "        #step environment\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #update agent\n",
    "        a.on_policy_td_sarsa(state, action, reward, next_state, i)\n",
    "        \n",
    "        #records\n",
    "        score+=reward\n",
    "        t+=1\n",
    "        if state==next_state: illegal_move+=1\n",
    "        if reward==-10: illegal_other+=1\n",
    "        \n",
    "        #move to next state\n",
    "        state = next_state\n",
    "        \n",
    "        #end if drop off at destination\n",
    "        if reward==20: break\n",
    "            \n",
    "    #record\n",
    "    scores.append(score)\n",
    "    ts.append(t)\n",
    "    illegal_moves.append(illegal_move)\n",
    "    illegal_others.append(illegal_other)\n",
    "    \n",
    "    if i > moving_nb:\n",
    "        moving_score = np.mean(scores[i-moving_nb:i])\n",
    "        moving_scores.append(moving_score)\n",
    "    else:\n",
    "        moving_scores.append(0)\n",
    "        \n",
    "    #break if solved\n",
    "    if moving_scores[-1] > solved_score: \n",
    "        print(f'Solved at Play {i}: {datetime.now() - start_time} Moving average: {moving_scores[-1]}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c14123",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(moving_scores[:2500])\n",
    "print(f'100-episode average score: {moving_scores[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e121a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts)\n",
    "print(f'100-episode average timesteps: {np.mean(ts[:-100])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033bcc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(illegal_moves)\n",
    "plt.plot(illegal_others)\n",
    "print(f'Illegal moves: {illegal_moves[-1]}; Illegal drop-offs/pickups: {illegal_others[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced55c35",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "For every **timestep**\n",
    "* **Step 1** Given state $s_t$, choose an action $a_t$ using epsilon greedy policy\n",
    "* **Step 2** Update the action value $Q(s_t,a_t)$ using the following equation:\n",
    "\n",
    "$Q(s_t,a_t) = Q(s_t,a_t) + \\alpha (R_{t+1} + \\gamma max_{a}Q(s_{t+1},a) - Q(s_t,a_t))$\n",
    "\n",
    "where $max_{a}Q(s_{t+1},a)$ is the highest action value given state $s_{t+1}$. \n",
    "\n",
    "This is very similar to SARSA except for the fact that we will always choose the best action value as opposed to epsilon-greedy action selection of SARSA. For this reason, SARSA is called **on-policy** and Q-learning **off-policy**.\n",
    "\n",
    "Q2\n",
    "\n",
    "**Implement** \n",
    "\n",
    "`off_policy_td_q_learning` method inside `taxi_v3_agent.py` file.\n",
    "\n",
    "Then run the following code blocks to generate the outputs and plots. Discuss the output results in brief. \n",
    "\n",
    "**Are there any significant differences in terms of the results between onpolicy and offpolicy TD Learning based on the results especially in terms of speed? Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf90f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "a = TaxiAgent(env, gamma = 0.8, alpha = 1e-1,\n",
    "        start_epsilon = 1, end_epsilon = 1e-2, epsilon_decay = 0.999)\n",
    "\n",
    "scores = []\n",
    "ts = []\n",
    "illegal_moves = []\n",
    "illegal_others = []\n",
    "moving_scores = []\n",
    "moving_nb = 100\n",
    "solved_score = 8.5\n",
    "start_time = datetime.now()\n",
    "\n",
    "for i in trange(500000):\n",
    "    #for the record\n",
    "    score = 0\n",
    "    t= 0\n",
    "    illegal_move = 0\n",
    "    illegal_other = 0\n",
    "    \n",
    "    #initiate state\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        \n",
    "        #get action\n",
    "        action = a.select_action(state, a.get_epsilon(i))\n",
    "        \n",
    "        #step environment\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #update agent\n",
    "        a.off_policy_td_q_learning(state, action, reward, next_state)\n",
    "        \n",
    "        #records\n",
    "        score+=reward\n",
    "        t+=1\n",
    "        if state==next_state: illegal_move+=1\n",
    "        if reward==-10: illegal_other+=1\n",
    "        \n",
    "        #move to next state\n",
    "        state = next_state\n",
    "        \n",
    "        #end if drop off at destination\n",
    "        if reward==20: break\n",
    "            \n",
    "    #record\n",
    "    scores.append(score)\n",
    "    ts.append(t)\n",
    "    illegal_moves.append(illegal_move)\n",
    "    illegal_others.append(illegal_other)\n",
    "    \n",
    "    if i > moving_nb:\n",
    "        moving_score = np.mean(scores[i-moving_nb:i])\n",
    "        moving_scores.append(moving_score)\n",
    "    else:\n",
    "        moving_scores.append(0)\n",
    "        \n",
    "    #break if solved\n",
    "    if moving_scores[-1] > solved_score: \n",
    "        print(f'Solved at Play {i}: {datetime.now() - start_time} Moving average: {moving_scores[-1]}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores)\n",
    "plt.plot(moving_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episodes')\n",
    "print(f'100-episode average score: {moving_scores[-1]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6674b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts)\n",
    "plt.ylabel('Time')\n",
    "plt.xlabel('Episodes')\n",
    "print(f'100-episode average timesteps: {np.mean(ts[:-100])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b62105",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(illegal_moves)\n",
    "plt.plot(illegal_others)\n",
    "print(f'Illegal moves: {illegal_moves[-1]}; Illegal drop-offs/pickups: {illegal_others[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e9c21f",
   "metadata": {},
   "source": [
    "**Q3**\n",
    "\n",
    "Based on what we have learned, would Monte Carlo be a better method than TD learning for this game? Justify your answer using one or two points of arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f6883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
