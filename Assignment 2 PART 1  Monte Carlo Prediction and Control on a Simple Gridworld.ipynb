{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from simple_grid import simple_grid as gridworld\n",
    "from simple_grid_agent import GridworldAgent as Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read through all the classes and functions defined inside `simple_grid` environment and `GridworldAgent` to familiarize yourself with the details of this assignment.\n",
    "\n",
    "Consider a simple gridworld where actions do not result in deterministic state changes. We specify that there is a $20\\%$ probability that the selected action would result in a stochastic state transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stochastic environment\n",
    "env = gridworld(wind_p=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following set of commands will help you familiarize with different components of the gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Reward For each Tile \n",
      "\n",
      "\n",
      "----------\n",
      "0 |0 |0 |\n",
      "----------\n",
      "0 |-5 |5 |\n",
      "----------\n",
      "0 |0 |0 |"
     ]
    }
   ],
   "source": [
    "print('\\n Reward For each Tile \\n')\n",
    "env.print_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the set of possible actions for the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Set of possible actions in numerical form. These are actual inputs to the gridworld agent \n",
      "\n",
      "[0 1 2 3]\n",
      "\n",
      " Set of possible actions in the grid in text form. They map 1 to 1 from numbers above to direction \n",
      "\n",
      "['U' 'L' 'D' 'R']\n"
     ]
    }
   ],
   "source": [
    "print('\\n Set of possible actions in numerical form. These are actual inputs to the gridworld agent \\n')\n",
    "print(env.action_space)\n",
    "\n",
    "print('\\n Set of possible actions in the grid in text form. They map 1 to 1 from numbers above to direction \\n')\n",
    "print(env.action_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a policy which tries to reach the goal state(+5) as fast as possible. Below we define the policy to evaluate the state values for this policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Policy: Fastest Path to Goal State(Does not take reward into consideration) \n",
      "\n",
      "\n",
      "----------\n",
      "R |R |D |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |U |U |"
     ]
    }
   ],
   "source": [
    "#stochastic environment\n",
    "env = gridworld(wind_p=0.2)\n",
    "\n",
    "#initial policy\n",
    "policy_fast = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 2,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "\n",
    "#stochastic agent - epsilon greedy with decays\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "print('\\n Policy: Fastest Path to Goal State(Does not take reward into consideration) \\n')\n",
    "a.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**\n",
    "\n",
    "Implement the `get_v` and `get_q` methods to estimate the state value and state-action value in `simple_grid_agent.py`. These may be used later on for debugging your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2** \n",
    "\n",
    "The Monte Carlo rollout itself has been implemented in `simple_grid_agent.py` inside the `run_episode` method.\n",
    "\n",
    "**Implement** \n",
    "\n",
    "First-visit as well as any-visit Monte Carlo state-value estimation equations inside `mc_predict_v` in `simple_grid_agent.py`.\n",
    "These have been discussed in class. Refer to Sutton and Barto Chapter 5 for further details to implement them.\n",
    "\n",
    "Test and report inside this notebook the results using the following commands. Are there sufficient differences in the state values under anyvisit and firstvisit MC Prediction? Why?\n",
    "\n",
    "**ANS: After resetting the action state it could be observed that the first visit and any visit yields somewhere around similar values for each states. Since both states leads to a convergence at inf, and 10,000 iterations are large enough for the this problem it seems like both the method reach to convergence.**\n",
    "\n",
    "NB: assume anyvist and everyvisit to be interchangeable terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " State Values for first_visit MC state estiamtion \n",
      "\n",
      "\n",
      "---------------\n",
      "-1.0 |0.8 |2.8 |\n",
      "---------------\n",
      "-3.6 |1.9 |0 |\n",
      "---------------\n",
      "-3.9 |-3.3 |2.5 |\n",
      " State Values for any_visit MC state estiamtion \n",
      "\n",
      "\n",
      "---------------\n",
      "-1.1 |0.8 |2.9 |\n",
      "---------------\n",
      "-3.7 |1.9 |0 |\n",
      "---------------\n",
      "-4.0 |-3.4 |2.5 |"
     ]
    }
   ],
   "source": [
    "# reset agent to compute first visit\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "# evaluate state values for policy_fast for both first-vist and any-vist\n",
    "print('\\n State Values for first_visit MC state estiamtion \\n')\n",
    "a.mc_predict_v()\n",
    "a.print_v()\n",
    "\n",
    "\n",
    "#Reset the agent to compute anyvisit.\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "print('\\n State Values for any_visit MC state estiamtion \\n')\n",
    "a.mc_predict_v(first_visit=False)\n",
    "a.print_v()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3** \n",
    "\n",
    "The Monte Carlo rollout itself has been implemented in `simple_grid_agent.py` inside the `run_episode` method.\n",
    "\n",
    "**Implement** \n",
    "\n",
    "First-visit as well as any-visit Monte Carlo state-action value estimation equations inside `mc_predict_q` in `simple_grid_agent.py`\n",
    "These have been discussed in class. Refer to Sutton and Barto Chapter 5 for further details to implement them.\n",
    "\n",
    "Test and report inside this notebook the results using the following commands. Are there sufficient differences in the state values under anyvisit and firstvisit MC Q value Prediction? Why?\n",
    "\n",
    "**ANS: After resetting the action state it could be observed that the first visit and any visit yields somewhere around similar values for each states. Since both states leads to a convergence at inf, and 10,000 iterations are large enough for the this problem it seems like both the method reach to convergence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " State action Values for first_visit MC state action estiamtion \n",
      "\n",
      "\n",
      " Actions ['U' 'L' 'D' 'R'] \n",
      "\n",
      "(2, 0) [-4.16750273 -4.51888798 -4.36521357 -3.79427285]\n",
      "(2, 1) [-3.66475661 -4.47381768 -4.02167861  1.0279243 ]\n",
      "(1, 1) [-0.4765078  -4.22739671 -3.98085441  3.332586  ]\n",
      "(0, 0) [-1.22337933 -2.66102216 -4.68203987 -0.82394693]\n",
      "(0, 1) [-0.83928958 -2.35159725 -3.97763362  1.48407266]\n",
      "(0, 2) [ 1.45533301 -0.47113593  3.30069162  1.58107833]\n",
      "(2, 2) [ 3.31022766 -4.0348658   0.94273806  1.30462188]\n",
      "(1, 0) [-2.47429108 -4.26871391 -4.52783806 -3.70494007]\n",
      "(1, 2) [0. 0. 0. 0.]\n",
      "\n",
      " State action Values for any_visit MC state action estiamtion \n",
      "\n",
      "\n",
      " Actions ['U' 'L' 'D' 'R'] \n",
      "\n",
      "(2, 0) [-3.95821625 -4.45990965 -4.6544496  -3.7249584 ]\n",
      "(1, 0) [-1.75911673 -4.03550175 -4.62940037 -3.39973448]\n",
      "(2, 1) [-3.55405506 -4.54431308 -3.80355254  1.0367748 ]\n",
      "(2, 2) [ 3.28200222 -4.02699107  1.07404017  0.95232936]\n",
      "(1, 1) [-0.47115175 -4.19427461 -3.80856445  3.33069783]\n",
      "(0, 2) [ 1.50983023 -0.6416251   3.35934537  1.43119667]\n",
      "(0, 0) [-1.93558584 -1.65290225 -3.95357269 -0.47793473]\n",
      "(0, 1) [-1.14793702 -1.71525368 -3.57976782  1.57445121]\n",
      "(1, 2) [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Reset Agent for the first value prediction\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "# evaluate state action values for policy_fast\n",
    "print('\\n State action Values for first_visit MC state action estiamtion \\n')\n",
    "a.mc_predict_q()\n",
    "print('\\n Actions', env.action_text, '\\n')\n",
    "for i in a.q: print(i,a.q[i])\n",
    "\n",
    "#reset agent for any visit or multi visit\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "# evaluate state action values for policy_fast\n",
    "print('\\n State action Values for any_visit MC state action estiamtion \\n')\n",
    "a.mc_predict_q(first_visit=False)\n",
    "print('\\n Actions', env.action_text, '\\n')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**\n",
    "\n",
    "Now we implement Monte Carlo control using state-action values. \n",
    "\n",
    "**Implement**\n",
    "\n",
    "Complete the snippet in `mc_control_q` inside `simple_grid_agent.py`\n",
    "\n",
    "Test and report inside this notebook the results using the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "L |R |D |\n",
      "----------\n",
      "U |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      " Actions: {env.action_text} \n",
      "\n",
      "(2, 0) [-4.04658836 -5.34450892 -4.90007443 -3.94344805]\n",
      "(2, 1) [-3.65231798 -4.7209081  -4.8003984   0.88024116]\n",
      "(1, 1) [-0.27084665 -4.68888426 -3.92718024  3.28968687]\n",
      "(1, 0) [-1.95307692 -4.10897386 -6.21430029 -3.74355583]\n",
      "(0, 1) [-0.12830397 -0.9415     -2.51577778  1.51963865]\n",
      "(0, 2) [ 1.36764705 -1.78515905  3.34775114  1.46954031]\n",
      "(0, 0) [-3.77986127 -0.715942   -2.45271267 -0.97200872]\n",
      "(2, 2) [ 3.29859775 -4.68125002  1.06604     1.78871429]\n",
      "(1, 2) [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#stochastic environment\n",
    "env = gridworld(wind_p=0.2)\n",
    "\n",
    "#initial policy\n",
    "policy_fast = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 2,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "\n",
    "#stochastic agent - epsilon greedy with decays\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "        start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "# Run MC Control\n",
    "a.mc_control_q(n_episode = 1000,first_visit=False)\n",
    "a.print_policy()\n",
    "\n",
    "print('\\n Actions: {env.action_text} \\n')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5**\n",
    "\n",
    "Bonus!\n",
    "\n",
    "**Implement**\n",
    "\n",
    "Greedy within The Limit of  Iinfinite Exploration MC Control in `mc_control_glie` function inside `simple_grid_agent.py`\n",
    "\n",
    "Test and report inside this notebook the results using the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "R |R |D |\n",
      "----------\n",
      "U |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      " Actions ['U' 'L' 'D' 'R'] \n",
      "\n",
      "(2, 0) [-4.80224638 -4.5433438  -4.61854381 -3.90056043]\n",
      "(2, 1) [-3.63059444 -5.04126388 -4.62309266  1.0182127 ]\n",
      "(2, 2) [ 3.20808258 -3.88123942  1.41190246  0.56157875]\n",
      "(1, 0) [-2.81703112 -5.26846742 -5.04917858 -4.06274592]\n",
      "(1, 1) [-0.45919111 -4.72471858 -4.25052735  3.21130167]\n",
      "(0, 0) [-5.17786799 -4.18227904 -4.35256601 -1.08375241]\n",
      "(0, 1) [-0.80037442 -2.2623703  -3.08151107  1.58831274]\n",
      "(0, 2) [ 1.44496469 -0.61447723  3.38559966  1.0171323 ]\n",
      "(1, 2) [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#stochastic environment\n",
    "env = gridworld(wind_p=0.2)\n",
    "\n",
    "#initial policy\n",
    "policy_fast = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 2,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "\n",
    "#stochastic agent - epsilon greedy with decays\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "        start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "a.mc_control_glie(n_episode = 1000)\n",
    "a.print_policy()\n",
    "print('\\n Actions', env.action_text, '\\n')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
